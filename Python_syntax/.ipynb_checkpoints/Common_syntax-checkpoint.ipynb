{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "# read excel\n",
    "df = pd.read_excel (r'Path where the Excel file is stored\\File name.xlsx', sheet_name='Type here the name of your Excel sheet')\n",
    "#print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "summary.to_csv('period_lacking_exchangerate.csv', header=True,index=None)\n",
    "# write excel\n",
    "file_name = 'QI-9178_signup&KYC_' + str(yesterday) + \".xlsx\"\n",
    "writer = pd.ExcelWriter(file_name, engine='xlsxwriter')\n",
    "sheet_2.to_excel(writer, sheet_name='Summary_total', index=None)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1_value.describe())\n",
    "print(data1_value.info())\n",
    "print(raw_data.shape)\n",
    "print(raw_data.columns)\n",
    "# Count NA values\n",
    "print('NUll: ',data1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df by filter columns\n",
    "data= raw_data.loc[:,('user_id','last_execution_date','nb_trading_days','delta_balance','total_deposit')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA with median, mode\n",
    "dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "# Fill NA = 0 \n",
    "data_combine.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df[\"A\"] = df[\"A\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "column_drop=['PassengerId','Cabin','Ticket'] \n",
    "data1.drop(column_drop, axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare condition and assign value in 1 column\n",
    "dataset['IsAlone']=1\n",
    "dataset['IsAlone'].loc[dataset['FamilySize']>1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "dataset['Title']= dataset['Name'].str.split(\", \", expand=True)[1].str.split(\". \",expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicate of each value in column\n",
    "data1['Title'].value_counts()\n",
    "\n",
    "# Check duplicate between columns\n",
    "shop_duplicate = raw_data[raw_data.duplicated(['shopid','item_name','item_description','price'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function for values in 1 column\n",
    "dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x]==True else x)\n",
    "kmeans_result['trade_volume'] = kmeans_result['trade_volume'].apply(lambda x: x/(10**6))\n",
    "\n",
    "# Join many columns into a new column in dataframe\n",
    "imei_flag_2['list_fraud'] = imei_flag_2[['list_fraud_rider_old','list_fraud_rider_new']].agg(lambda x: [j for i in x for j in i], axis=1)\n",
    "\n",
    "# create column list unique from 2 columns has list:\n",
    "imei_flag_df = merge_data.groupby(['imei_number'])['rank_order'].count()\n",
    "imei_flag_df = imei_flag_df.reset_index()\n",
    "imei_flag_df = imei_flag_df.loc[imei_flag_df['rank_order']>=2,:]\n",
    "black_imei = list(imei_flag_df['imei_number'])\n",
    "print(len(black_imei))\n",
    "imei_flag_2 = merge_data.loc[merge_data['imei_number'].isin(black_imei),:]\n",
    "imei_flag_2 = imei_flag_2.groupby('imei_number').agg(\n",
    "    number_switch_times = pd.NamedAgg(column= 'ord_date', aggfunc='count'),\n",
    "    number_fraud_rider = pd.NamedAgg(column= 'riderId', aggfunc='count'),\n",
    "    number_order = pd.NamedAgg(column= 'rank_order', aggfunc='sum'),\n",
    "    list_fraud_rider_old = pd.NamedAgg(column= 'riderId', aggfunc='unique'),\n",
    "    list_fraud_rider_new = pd.NamedAgg(column= 'riderId_new', aggfunc='unique'),  \n",
    ")\n",
    "imei_flag_2['list_fraud'] = imei_flag_2[['list_fraud_rider_old','list_fraud_rider_new']].agg(lambda x: [j for i in x for j in i], axis=1)\n",
    "imei_flag_2['list_fraud'] = imei_flag_2['list_fraud'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantiles\n",
    "quantiles = user_1.quantile(q=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "quantiles = pd.DataFrame(data =quantiles, columns=['recency','fre_cmt','count_thread'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "quantiles = quantiles.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data cac cot thanh 1 cot\n",
    "user_1['RFM'] = user_1['Recency_Group'].map(str) + user_1['Frequency_Group'].map(str) + user_1['Thread_Group'].map(str)\n",
    "\n",
    "# List in each rows/columns \n",
    "imei_flag_2['list_fraud'] = imei_flag_2[['list_fraud_rider_old','list_fraud_rider_new']].agg(lambda x: [j for i in x for j in i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter nhieu dieu kien\n",
    "longtimereturn_sample = user_1[(user_1['Recency_Group']>=4) & (user_1['Frequency_Group']==1)]\n",
    "\n",
    "result_df= result_df.loc[:,['currency','captured_date','uf_get_usd_rate_for_currency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by\n",
    "count_group= user_1.groupby(['Recency_Group','Frequency_Group'])['user_id'].count()\n",
    "\n",
    "kmeans_group = kmeans_group.groupby('Cluster').agg({'user_id': 'nunique',\n",
    "                                                      'recency':'mean',\n",
    "                                                      'nb_trading_days':'mean',\n",
    "                                                      'open_balance':'mean',\n",
    "                                                     'sum_DW':'mean',\n",
    "                                                     'trade_volume':'mean'})\n",
    "\n",
    "data.groupby('month', as_index=False).agg({\"duration\": \"sum\"})\n",
    "\n",
    "sum_bnf.groupby('post_code')['items'].sum().to_frame('sum_total')\n",
    "\n",
    "data[data['item'] == 'call'].groupby('month').agg(\n",
    "    # Get max of the duration column for each group\n",
    "    max_duration=('duration', max),\n",
    "    # Get min of the duration column for each group\n",
    "    min_duration=('duration', min),\n",
    "    # Get sum of the duration column for each group\n",
    "    total_duration=('duration', sum),\n",
    "    # Apply a lambda to date column\n",
    "    num_days=(\"date\", lambda x: (max(x) - min(x)).days)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby and join list\n",
    "In [63]: df\n",
    "Out[63]: \n",
    "   a          b    c\n",
    "0  1  [1, 2, 3]  foo\n",
    "1  1     [2, 5]  bar\n",
    "2  2     [5, 6]  baz\n",
    "\n",
    "\n",
    "In [64]: df.groupby('a').agg({'b': 'sum', 'c': lambda x: ' '.join(x)})\n",
    "Out[64]: \n",
    "         c                b\n",
    "a                          \n",
    "1  foo bar  [1, 2, 3, 2, 5]\n",
    "2      baz           [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by and rename.\n",
    "# Group by create set \n",
    "sum_bnf.groupby('post_code')['items'].sum().to_frame('sum_total')\n",
    "sum_bnf.groupby('post_code')['items'].sum().to_frame('sum_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "count_group= pd.DataFrame(count_group).reset_index()\n",
    "# reset index\n",
    "result_df = result_df.reset_index()\n",
    "selected_rider_metric.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "user_kmeans = pd.DataFrame(data=user_kmeans, columns=['user_id','username','last_activity'])\n",
    "user_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "data = data.rename(columns={'uf_get_usd_rate_for_currency':'exchange_rate'})\n",
    "\n",
    "# Get columns of dataframe\n",
    "cols_plot = reshape_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby\n",
    "top_3_pre = top_3_pre.groupby('shopid')['itemid'].nunique()\n",
    "# Sort value \n",
    "top_3_pre.sort_values(by=[\"itemid\"], axis=0, ascending=False, inplace=True, na_position ='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe\n",
    "reshape_data_plt = reshape_data.pivot(index='year_month', columns= 'currency', values='exchange_rate')\n",
    "\n",
    "merged_df = merged_df.pivot(index='time', columns= 'app_type', values='sessions')\n",
    "merged_df= merged_df.reset_index().rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datetime to strptime\n",
    "check['last_execution_date'] = check['last_execution_date'].apply(lambda x : dt.datetime.strptime(x, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# Get year/month/day\n",
    "data['year']=data['captured_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").strftime(\"%Y\"))\n",
    "\n",
    "\n",
    "df['col'] = pd.to_datetime(df['col'])\n",
    "raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object into code\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "label = LabelEncoder()\n",
    "temp['Sex_code']=label.fit_transform(temp['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived', 'Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare']\n",
      "['Survived', ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare']]\n"
     ]
    }
   ],
   "source": [
    "# Expand List\n",
    "target = ['Survived']\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare']\n",
    "data1_xy =  target + data1_x_calc\n",
    "target.append(data1_x_calc)\n",
    "print(data1_xy)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Y': 1, 'N': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dict with 2 list\n",
    "encode_dict = dict(zip(list(\"YYNNYN\"),[1,1,0,0,1,0]))\n",
    "encode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List keys in Dict\n",
    "d = {'A':[1,2,3],\n",
    "    'B':[4,5,6],\n",
    "     'C':[7,8,9]\n",
    "    }\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time execute\n",
    "t0 = datetime.datetime.now()\n",
    "t1 = datetime.datetime.now()\n",
    "print('Execution time: {}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data frame\n",
    "result_row = pd.DataFrame(row, columns=cols)\n",
    "\n",
    "# Create dataframe from Dict\n",
    "summary = pd.DataFrame.from_dict(dict_nacur, orient='index', columns=['month_null_data'])\n",
    "\n",
    "a = [{\"user_id\": \"abc\", \"fraud_order\": \"xyaj0101\", \"rider_id\": \"hung\"}, \n",
    "     {\"user_id\": \"cdf\", \"fraud_order\": \"x343101\", \"rider_id\": \"nhan\"}]\n",
    "df = pd.DataFrame(a, columns = [\"user_id\",\"fraud_order\",\"rider_id\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter isin and not isin\n",
    "new_df.loc[raw_data.index.isin(shop_duplicate.index), 'is_duplicated'] = True   \n",
    "new_df.loc[~raw_data.index.isin(shop_duplicate.index), 'is_duplicated'] = False  \n",
    "data_use = data_use.loc[~data_use[\"riderId\"].isin(rider_1dvc)==True,:] \n",
    "selected_rider_metric = rawdata.loc[rawdata['rider_metric_merchant'].isin(selected_merchant)==True,:]\n",
    "\n",
    "df.countries.isin(countries)\n",
    "df[df.countries.isin(countries)]\n",
    "df[~df.countries.isin(countries)]\n",
    "\n",
    "# select distinct cols in df (remove duplicate and reset index):\n",
    "rider_metric = rawdata.loc[:,['rider_metric_merchant','riderId','userId','merchant_id','merchant_longtitute','merchant_lattitute']]\n",
    "rider_metric = rider_metric.drop_duplicates()\n",
    "rider_metric_rawdata.reset_index(drop=True, inplace=True)\n",
    "print(len(rider_metric))\n",
    "\n",
    "# remove duplicate of a list:\n",
    "mylist = [\"a\", \"b\", \"a\", \"c\", \"c\"]\n",
    "mylist = list(dict.fromkeys(mylist))\n",
    "print(mylist)\n",
    "\n",
    "# delete null:\n",
    "data_use = rawdata.loc[(rawdata['riderId'].isnull()==False)&(rawdata['dvc_id'].isnull()==False),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row bind dataframe\n",
    "result_df=result_df.append(result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open text file and write\n",
    "exists = os.path.isfile(filename)\n",
    "if exists:\n",
    "  print('Exists: '+filename)\n",
    "else:\n",
    "  print('Create: '+filename)\n",
    "  with open(filename,'w') as outfile:\n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/map'\n",
    "    parameters = {}\n",
    "    session.headers.update(headers)\n",
    "    response = session.get(url, params=parameters)\n",
    "    data = json.loads(response.text)\n",
    "    for d in data[\"data\"]:\n",
    "      cryptos_list = cryptos_list + ',' + d[\"symbol\"]\n",
    "    outfile.write(cryptos_list.strip(','))\n",
    "\n",
    "print('Get Cryptos from Coinmarket: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA\n",
    "import scipy.stats as st\n",
    "st.f_oneway(sample1, sample2, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation and Correlation computation\n",
    "import scipy.stats as st\n",
    "st.pearsonr(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/exploratory-data-analysis-in-python-set-2/\n",
    "# Plot BARPPLOT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax1 = plt.subplots() \n",
    "fig.set_size_inches(15,  9) \n",
    "  \n",
    "  \n",
    "ax1 = sns.barplot(x =\"State\", y =\"Population\",  \n",
    "                  data = data.sort_values('MurderRate'),  \n",
    "                                        palette =\"Set2\") \n",
    "  \n",
    "ax1.set(xlabel ='States', ylabel ='Population In Millions') \n",
    "ax1.set_title('Population in Millions by State', size = 20) \n",
    "  \n",
    "plt.xticks(rotation =-90) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "Population_std = data.Population.std() \n",
    "print (\"Population std : \", Population_std) \n",
    "\n",
    "#Variance\n",
    "Population_var = data.Population.var() \n",
    "print (\"Population var : \", Population_var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA: axis la truc, cot\n",
    "# it is along rows i.e. axis = 0 \n",
    "dframe.dropna(inplace = True) \n",
    "print(dframe) \n",
    "  \n",
    "# if axis is equal to 1 \n",
    "dframe.dropna(axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Program to create\n",
    "# a data type object\n",
    "import numpy as np\n",
    " \n",
    "# First Array\n",
    "arr1 = np.array([[4, 7], [2, 6]], \n",
    "                 dtype = np.float64)\n",
    "                  \n",
    "# Second Array\n",
    "arr2 = np.array([[3, 6], [2, 8]], \n",
    "                 dtype = np.float64) \n",
    " \n",
    "# Addition of two Arrays\n",
    "Sum = np.add(arr1, arr2)\n",
    "print(\"Addition of Two Arrays: \")\n",
    "print(Sum)\n",
    " \n",
    "# Addition of all Array elements\n",
    "# using predefined sum method\n",
    "Sum1 = np.sum(arr1)\n",
    "print(\"\\nAddition of Array elements: \")\n",
    "print(Sum1)\n",
    " \n",
    "# Square root of Array\n",
    "Sqrt = np.sqrt(arr1)\n",
    "print(\"\\nSquare root of Array1 elements: \")\n",
    "print(Sqrt)\n",
    " \n",
    "# Transpose of Array\n",
    "# using In-built function 'T'\n",
    "Trans_arr = arr1.T\n",
    "print(\"\\nTranspose of Array: \")\n",
    "print(Trans_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition in sql\n",
    "SELECT currency_id, usdrate,capturedate\n",
    "                    FROM \n",
    "                    (\n",
    "                    SELECT currency_id, usdrate,capturedate,\n",
    "                           row_number() OVER (PARTITION BY currency_id ORDER BY capturedate DESC) as nb_row\n",
    "                    FROM dm_fact.exc01_exchange_consolidation\n",
    "                    WHERE currency_id in (20,22,24)\n",
    "                    and capturedate < v_from_time\n",
    "                    ) as r \n",
    "                    WHERE nb_row =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3 -3]\n",
      "\n",
      " Elements are : \n",
      " [2 4 7]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(np.array([1, 3, -3]))\n",
    "arr = x[np.array([1, 3, -3])] \n",
    "print(\"\\n Elements are : \\n\",arr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "Modified array is:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Reshape array\n",
    "import numpy as geek\n",
    "a = geek.arange(12)\n",
    "print(a)\n",
    "a = a.reshape(3,4)\n",
    "print(a)\n",
    "b = a.T \n",
    "print('Modified array is:')\n",
    "for x in geek.nditer(b): \n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframe\n",
    "merge_gr2_rider = pd.merge(merge_gr2_rider, rider_2, left_on =\"dvc_id\", right_on=\"dvc_id\")\n",
    "\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "\n",
    "left: A DataFrame or named Series object.\n",
    "\n",
    "right: Another DataFrame or named Series object.\n",
    "\n",
    "on: Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and left_index and right_index are False, the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys.\n",
    "\n",
    "left_on: Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n",
    "\n",
    "right_on: Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n",
    "\n",
    "left_index: If True, use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series.\n",
    "\n",
    "right_index: Same usage as left_index for the right DataFrame or Series\n",
    "\n",
    "how: One of 'left', 'right', 'outer', 'inner'. Defaults to inner. See below for more detailed description of each method.\n",
    "\n",
    "sort: Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve performance substantially in many cases.\n",
    "\n",
    "suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y').\n",
    "\n",
    "copy: Always copy data (default True) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.\n",
    "\n",
    "indicator: Add a column to the output DataFrame called _merge with information on the source of each row. _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in 'left' DataFrame or Series, right_only for observations whose merge key only appears in 'right' DataFrame or Series, and both if the observation’s merge key is found in both.\n",
    "\n",
    "validate : string, default None. If specified, checks if merge is of specified type.\n",
    "\n",
    "“one_to_one” or “1:1”: checks if merge keys are unique in both left and right datasets.\n",
    "\n",
    "“one_to_many” or “1:m”: checks if merge keys are unique in left dataset.\n",
    "\n",
    "“many_to_one” or “m:1”: checks if merge keys are unique in right dataset.\n",
    "\n",
    "“many_to_many” or “m:m”: allowed, but does not result in checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby column to agg list\n",
    "df1 = df.groupby('a')['b'].apply(list).reset_index(name='new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to dict\n",
    "df.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to dict by index:\n",
    "a = user_segment_HCM.set_index(['cluster_id'])\n",
    "a = a.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row_1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row_2</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  2  3\n",
       "0  row_1  3  2  1  0\n",
       "1  row_2  a  b  c  d"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# convert dict to dataframe:\n",
    "data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n",
    "a = pd.DataFrame.from_dict(data, orient='index')\n",
    "#        0  1  2  3\n",
    "# row_1  3  2  1  0\n",
    "# row_2  a  b  c  d\n",
    "a = a.reset_index()\n",
    "a = a.rename(columns = {\"0\":\"T1\"})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and remove index\n",
    "deposit_df = deposit_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join string in dataframe:\n",
    "results_dvc_group = results.groupby(['deviceId_list','number_deviceId']).agg(\n",
    "    group_set = pd.NamedAgg(column='group_code', aggfunc= \",\".join))\n",
    "# Split and unique in line of 1 column dataframe:\n",
    "results_dvc_group['group_rider'] = results_dvc_group['group_rider'].str.split(',', expand=False).agg([set]).agg([list])\n",
    "# Replace string:\n",
    "results_dvc_group['deviceId_list'] = results_dvc_group['deviceId_list'].str.replace(\" \",\"\").str.replace(\"'\",\"\").str.replace(\"{\",\"\").str.replace(\"}\",\"\").str.split(',', expand=False).agg([set]).agg([list])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat 2 dataframe:\n",
    "df_train = pd.read_csv('train.csv').drop(columns=['trip_duration', 'dropoff_datetime'])\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df = pd.concat([df_train, df_test], sort=False, ignore_index=True)\n",
    "\n",
    "# Create new dataframe 50 highest & 50 lowest\n",
    "a = [data_combine.head(50),data_combine.tail(50)]\n",
    "# script_growth = pd.concat(data_combine.head(50),data_combine.tail(50))\n",
    "script_growth = pd.concat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get schema field of table on bigquery:\n",
    "# terminal: brew install jq\n",
    "# terminal: bq show --format=prettyjson baemin-vietnam:baemin.orders | jq '.schema.fields'\n",
    "\n",
    "# hash data:\n",
    "def hash_data(merged_df, *columns):\n",
    "    for col in columns:\n",
    "        merged_df[col] = merged_df[col] + \"hEh$0\"\n",
    "        merged_df[col] = merged_df[col].str.encode('utf-8').apply(lambda x: (hashlib.sha256(x).hexdigest().upper()))\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add append new column from dataframe to table bigquery \n",
    "\n",
    "new_df_cols_group = list(group_data_df.columns)\n",
    "def setdiff_sorted(array1, array2, assume_unique=True):\n",
    "    ans = np.setdiff1d(array1,array2,assume_unique).tolist()\n",
    "    if assume_unique:\n",
    "        return sorted(ans)\n",
    "    return ans\n",
    "\n",
    "def update_table_schema(new_df_cols,dataset_name,task_name):\n",
    "    bigquery_client = bigquery.Client.from_service_account_json(\"data_service.json\")\n",
    "    dataset_ref = bigquery_client.dataset(dataset_name)\n",
    "    table_ref = dataset_ref.table(task_name)\n",
    "    table = bigquery_client.get_table(table_ref)\n",
    "    new_schema = list(table.schema)\n",
    "    list_schema = []\n",
    "    i = 0\n",
    "    for i in range (0, len(new_schema)):\n",
    "        list_schema.append(str(new_schema[i]).split(',')[0][13:-1])\n",
    "    added_fields = setdiff_sorted(new_df_cols, list_schema)\n",
    "    for field in added_fields:\n",
    "        new_schema.append(bigquery.SchemaField(f'{field}', 'STRING'))\n",
    "    table.schema = new_schema\n",
    "    table = bigquery_client.update_table(table, ['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_items'>\n"
     ]
    }
   ],
   "source": [
    "a = {\"abc\":[1,2,4,5],\"def\":[5,7,4,6]}\n",
    "a.items()\n",
    "print(type(a.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort list of tuple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download zip file from url\n",
    "zip_file_url = \"https://appboy-03-data-export.s3.amazonaws.com/ds2PZszWJp5MEC_u9qJWp543bQ7zfaOJyqkyIm3-Pz4.zip\"\n",
    "save_path = '/Users/linh.trinh/Documents/GitHub/DWH_project/BAEMIN/Braze/extract_file'\n",
    "\n",
    "import requests, zipfile, io\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop internal a list:\n",
    "def list_merchant_accept_distance_by_rider_metric(long_lat,rider_metric_id, require_dist):\n",
    "    origin_mc_list = list(long_lat[rider_metric_id].keys())\n",
    "    # distance_dict = {}\n",
    "    selected_merchant = []\n",
    "    print(\"rider_metric_id: \",rider_metric_id)\n",
    "    print(\"origin_mc_list: \",origin_mc_list)\n",
    "    while True:\n",
    "        print(\"len_origin_mc_list begin: \",len(origin_mc_list))\n",
    "        merchant_a = origin_mc_list[0]\n",
    "        remain_list = origin_mc_list[1:len(origin_mc_list)]\n",
    "\n",
    "        for merchant_b in remain_list:\n",
    "            origin = long_lat[rider_metric_id][merchant_a]\n",
    "            destination = long_lat[rider_metric_id][merchant_b]\n",
    "            d = distance(origin, destination)\n",
    "#             print(\"merchant_a:{} & merchant_b:{} & distance: {}\".format(merchant_a,merchant_b,d))\n",
    "            if d >= require_dist: \n",
    "#                 d_2merchant = {(merchant_a,merchant_b):d }\n",
    "#                 distance_dict.update(d_2merchant)\n",
    "                selected_merchant.extend([merchant_a,merchant_b])\n",
    "        del origin_mc_list[0]\n",
    "#         print(\"distance_dict:\",distance_dict)\n",
    "        if len(origin_mc_list)==0:\n",
    "            print(\"-\"*60)\n",
    "            break\n",
    "    selected_merchant = list(dict.fromkeys(selected_merchant))\n",
    "    print(\"len_selected_merchant list\",len(selected_merchant))\n",
    "    return selected_merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection set: \n",
    "intersec_dvc = set_a.intersection(set_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asd', {'a': 1, 'b': 2})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dictionary to tuple:\n",
    "total_items_by_bnf_post ={\"asd\":{\"a\":1,\"b\":2}}\n",
    "total_items_by_bnf_post = list(total_items_by_bnf_post.items())\n",
    "total_items_by_bnf_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate list or tuple:\n",
    "a = (\"abd\",\"efg\",\"reg\")\n",
    "for i, j in enumerate(a):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum\n",
    "merge_data = practices.merge(scripts, left_on = 'code', right_on = 'practice').reset_index()\n",
    "sum_bnf =  merge_data.groupby(['post_code','bnf_name'])['items'].sum().reset_index()\n",
    "sum_bnf = sum_bnf.sort_values('items',ascending = False).groupby('post_code').first()\n",
    "sum_bnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column type to string \n",
    "merged_full_day_df[col] = merged_full_day_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tuple from dataframe\n",
    "script_growth = pd.concat(a)\n",
    "script_growth = list(script_growth.itertuples(index=False, name=None))\n",
    "script_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-01-January'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt \n",
    "a= dt.datetime(1970, 1, 1).strftime('%Y-%d-%B')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_func() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b2db4537258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: my_func() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def my_func(n1, n2): \n",
    "    return n1 + n2 \n",
    "my_func(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_number/rank python \n",
    "rawdata['rank_order'] = rawdata.groupby('imei_number')['ord_date'].rank(method='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "# save_fig(\"attribute_histogram_plots\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw correlation \n",
    "\n",
    "#from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "# Pick the top 3 features that correlate with median_house_value most\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLlite\n",
    "import sqlite3\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn = sqlite3.connect('TestDB2.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE CARS (Brand text, Price number)')\n",
    "conn.commit()\n",
    "\n",
    "Cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "\n",
    "df = DataFrame(Cars, columns= ['Brand', 'Price'])\n",
    "df.to_sql('CARS', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT Brand, sum(price) FROM CARS\n",
    "GROUP BY Brand\n",
    "          ''')\n",
    "\n",
    "df = DataFrame(c.fetchall(), columns=['Brand','Price'])    \n",
    "print (df)\n",
    "\n",
    "#c.execute('DROP TABLE CARS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join list \n",
    "ops = '|'.join(opioids)\n",
    "\n",
    "# join string in query sql:\n",
    "query = \"\"\" \n",
    "    WITH order_data as\n",
    "    (\n",
    "    SELECT cast(datetime(createdAt, 'Asia/Saigon') as date) as order_date\n",
    "    , datetime(createdAt, 'Asia/Saigon') ord_dt\n",
    "    , _id as order_id \n",
    "    , riderId\n",
    "    , dishTotalPrice\n",
    "    , userId , status\n",
    "    FROM `baemin-vietnam.secret.baemin_orders` \n",
    "    WHERE cast(datetime(createdAt, 'Asia/Saigon') as date) = cast('\"\"\"+start_date+\"\"\"' as date)\n",
    "    AND status = 'DELIVERED'\n",
    "    )\n",
    "    , last_order AS\n",
    "    (\n",
    "        SELECT *\n",
    "        FROM\n",
    "        (\n",
    "            SELECT *\n",
    "            , ROW_NUMBER () OVER(PARTITION BY riderId, order_date ORDER BY ord_dt asc) as rank_order\n",
    "            FROM order_data\n",
    "            ) as ord_data\n",
    "        WHERE rank_order in (\"\"\"+\",\".join([str(x) for x in rank_order])+\"\"\")\n",
    "        AND cast(dishTotalPrice as int64) < \"\"\"+str(fraud_price)+\"\"\"\n",
    "    )\n",
    "    SELECT o.*, dishes.name as dish_name\n",
    "    FROM last_order o\n",
    "    LEFT JOIN `baemin-vietnam.secret.baemin_dishes` A ON A._id = o.order_id\n",
    "    CROSS JOIN UNNEST(A.dishes) dishes\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove left blank and right blank in string:\n",
    "txt = \"     banana     \"\n",
    "x = txt.rstrip().lstrip()\n",
    "print(x)\n",
    "print(\"of all fruits\", x, \"is my favorite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/59633091/python-subtract-cumulative-column-for-python-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pkl')\n",
    "print(\"Model dumped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters = model_clus4.predict(RFM_norm2)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict proba & predict \n",
    "model = load_model()\n",
    "y_test_proba = model.predict_proba(X_test)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_cls_1_proba = y_test_proba[:,1]\n",
    "y_test_cls_1_pred = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORT FRANCHISE \n",
    "dict_deposit_all_affiliate = {}\n",
    "sum_deposit_all_affiliate = []\n",
    "def cal_total_deposit_user(user_id,dict_deposit_all_affiliate,sum_deposit_all_affiliate):\n",
    "    try:\n",
    "        deposit_userA = balance_volume_dw[user_id]['deposit_usdt']\n",
    "    except:\n",
    "        deposit_userA = 0\n",
    "    level_userA = dict_user[user_id]\n",
    "    child_listA = dict_child[user_id]\n",
    "    if len(child_listA)==0:\n",
    "        total_deposit_userA = deposit_userA\n",
    "        if user_id not in dict_deposit_all_affiliate.keys():\n",
    "            sum_deposit_all_affiliate.append( {'user_id':user_id,'level':level_userA, 'deposit_usdt':total_deposit_userA})\n",
    "            dict_deposit_all_affiliate[user_id] = {'user_id':user_id,'level':level_userA, 'deposit_usdt':total_deposit_userA}\n",
    "        print(\"user_id: {} & level_userA= {} & af_deposit_userA = {} \".format(user_id,level_userA,total_deposit_userA))\n",
    "        return total_deposit_userA\n",
    "    else:\n",
    "        total_deposit_userA = deposit_userA\n",
    "        sum_deposit_children = 0 \n",
    "        for child_id in child_listA:\n",
    "            try:\n",
    "                deposit_child = balance_volume_dw[child_id]['deposit_usdt']\n",
    "            except:\n",
    "                deposit_child = 0\n",
    "            \n",
    "            level_child_id = dict_user[child_id]\n",
    "            sum_deposit_children += cal_total_deposit_user(child_id,dict_deposit_all_affiliate,sum_deposit_all_affiliate)\n",
    "        total_deposit_userA +=sum_deposit_children    \n",
    "        if user_id not in dict_deposit_all_affiliate.keys():\n",
    "            sum_deposit_all_affiliate.append({'user_id':user_id,'level':level_userA, 'deposit_usdt':total_deposit_userA})\n",
    "            dict_deposit_all_affiliate[user_id] = {'user_id':user_id,'level':level_userA, 'deposit_usdt':total_deposit_userA}\n",
    "        print(\"user_id: {} & level_userA= {} & af_deposit_userA = {} \".format(user_id,level_userA,total_deposit_userA))\n",
    "        return total_deposit_userA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', {'a': 124, 'b': 2343, 'c': 3444})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT FROM DICT TO LIST OF TUPLE \n",
    "total_items_by_bnf_post ={\"data\":{\"a\":124,\"b\":2343, \"c\":3444}}\n",
    "total_items_by_bnf_post = list(total_items_by_bnf_post.items())\n",
    "total_items_by_bnf_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP FROM RIGHT TO LEFT: LOOP TU PHAI SANG TRAI\n",
    "arr = [17,18,5,4,6,1]\n",
    "maxRight = -1\n",
    "for i in range(len(arr)-1, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 19, 23, 23, 29, 445662, tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "now_utc = dt.datetime.now(timezone('UTC'))\n",
    "now_pacific = now_utc.astimezone(timezone('US/Pacific'))\n",
    "now_pacific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert .py file to .exe\n",
    "# https://www.youtube.com/watch?v=zTV_HPFVSHE\n",
    "# tạo thư mục chứa file .py\n",
    "# vào commandline trỏ đến thư mục đó và gõ lệnh:\n",
    "# pyinstaller --onefile -w shared_investment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column of dataframe:\n",
    "def data_by_month(package,name):\n",
    "    df_1 = cat_accumulate_level(start_date_str,'2021-02-01', package)\n",
    "    df_2 = cat_accumulate_level(start_date_str,'2021-03-01', package)\n",
    "    df_3 = cat_accumulate_level(start_date_str,'2021-04-01', package)\n",
    "    df_4 = cat_accumulate_level(start_date_str,'2021-05-01', package)\n",
    "    df_5 = cat_accumulate_level(start_date_str,'2021-06-01', package)\n",
    "    df_6 = cat_accumulate_level(start_date_str,'2021-07-01', package)\n",
    "    df_7 = cat_accumulate_level(start_date_str,'2021-08-01', package)\n",
    "    df_8 = cat_accumulate_level(start_date_str,'2021-09-01', package)\n",
    "    df_9 = cat_accumulate_level(start_date_str,'2021-10-01', package)\n",
    "    df_10 = cat_accumulate_level(start_date_str,'2021-11-01', package)\n",
    "    df_11 = cat_accumulate_level(start_date_str,'2021-12-01', package)\n",
    "    df_12 = cat_accumulate_level(start_date_str,'2022-01-01', package)\n",
    "    df = pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11,df_12], axis = 1)\n",
    "    df= df.reset_index()\n",
    "    df.columns=['cat_level','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12']\n",
    "    df.to_csv(f'{name}.csv', header=True,index=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7060404758427895\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Create random number in range\n",
    "from random import uniform\n",
    "\n",
    "# randrange gives you an integral value\n",
    "irand = randrange(-1, 2)\n",
    "\n",
    "# uniform gives you a floating-point value\n",
    "frand = uniform(0.69, 0.71)\n",
    "\n",
    "print(frand)\n",
    "print(irand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop ngược list\n",
    "a = [1,2,4,5,7]\n",
    "for i in range(len(a) - 1, -1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare time with time:\n",
    "def convert_str_time(now_time, time_str):\n",
    "    block_at = dt.datetime.strptime(time_str, \"%H:%M:%S\")\n",
    "    block_at = now_time.replace(hour=block_at.time().hour, minute=block_at.time().minute, second=block_at.time().second, microsecond=0)\n",
    "    return block_at\n",
    "\n",
    "first_min_block = \"10:30:00\"\n",
    "first_max_block = \"12:30:00\"\n",
    "\n",
    "second_min_block = \"15:30:00\"\n",
    "second_max_block = \"16:00:00\"\n",
    "\n",
    "def check_time_to_sleep():\n",
    "    now_time = dt.datetime.now(pytz.timezone('Asia/Saigon')) \n",
    "    first_time_min = convert_str_time(now_time, first_min_block)\n",
    "    first_time_max = convert_str_time(now_time, first_max_block)\n",
    "    second_time_min = convert_str_time(now_time, second_min_block)\n",
    "    second_time_max = convert_str_time(now_time, second_max_block)\n",
    "    \n",
    "    if (now_time >= first_time_min) and (now_time < first_time_max):\n",
    "        dur_sleep = first_time_max - now_time\n",
    "        print(f\"Now is {now_time} and sleep = {dur_sleep}\")\n",
    "        sleep(dur_sleep.total_seconds())\n",
    "    elif (now_time >= second_time_min) and (now_time < second_time_max):\n",
    "        dur_sleep = second_time_max - now_time\n",
    "        print(f\"Now is {now_time} and sleep = {dur_sleep}\")\n",
    "        sleep(dur_sleep.total_seconds())\n",
    "    else:\n",
    "        print(\"No sleep\")\n",
    "    \n",
    "check_time_to_sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đây là apply cho cột mơi chỉ cần tính toán đơn giản\n",
    "kmeans_result['trade_volume'] = kmeans_result[‘trade_volume’].apply(lambda x: x/(10**6))\n",
    "\n",
    "# Đây là apply cho function\n",
    "RFM_km[‘segment_name’]= RFM_km[‘cluster_id’].apply(lambda x: name_segment_for_userid(x,cluster_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove final element of list ;\n",
    "fruits = ['apple', 'banana', 'cherry']\n",
    "fruits.pop()\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ailabtools.ailab_multiprocessing import pool_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "New List after pop :  [1, 2, 3, 4, 5] \n",
      "\n",
      "4\n",
      "('cat', 'bat')\n",
      "3\n",
      "New List after pop :  [1, 2] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pop:\n",
    "\n",
    "\n",
    "# Python3 program for pop() method \n",
    "  \n",
    "list1 = [ 1, 2, 3, 4, 5, 6 ] \n",
    "  \n",
    "# Pops and removes the last element from the list \n",
    "print(list1.pop()) \n",
    "  \n",
    "# Print list after removing last element \n",
    "print(\"New List after pop : \", list1, \"\\n\") \n",
    "  \n",
    "list2 = [1, 2, 3, ('cat', 'bat'), 4] \n",
    "  \n",
    "# Pop last three element \n",
    "print(list2.pop()) \n",
    "print(list2.pop()) \n",
    "print(list2.pop()) \n",
    "  \n",
    "# Print list \n",
    "print(\"New List after pop : \", list2, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe into list of dict by groupby 2 columns\n",
    "\n",
    "a= pd.DataFrame({\"user_id\":[\"abc\",\"abc\",\"def\",\"def\"],\"sesion_id\":[123,123,567,567],\"timestamp\":[1232,544,67567,2342],\"event_name\":[\"home\",\"search\",\"home\",\"search\"]})\n",
    "b = a.groupby(['user_id','sesion_id']).agg({\"timestamp\": lambda t: list(t),\"event_name\": lambda t: list(t)})\n",
    "b = b.apply(lambda x: [{'event_name': b, 'timestamp': a} for a, b in zip(x[0], x[1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert string to datetime\n",
    "data['payDateTime'] = data['payDate'].apply(lambda x: dt.datetime.utc_dt.strptime(x[0:14],'%Y%m%d%H%M%S') if x!='nan' else None)\n",
    "data['transaction_date'] = data['payDate'].apply(lambda x: dt.datetime.utc_dt.strptime(x[0:14],'%Y%m%d%H%M%S').strftime(\"%Y-%m-%d\") if x!='nan' else loading_date_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame({\"user\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop into dataframe row:\n",
    "# convert dataframe to dict list \n",
    "shop = defaultdict(list)\n",
    "for i, row in df.iterrows():\n",
    "    orderid = row[0]\n",
    "    shopid = row[1]\n",
    "    userid = row[2]\n",
    "    event_time = row[3]\n",
    "    event_time = dt.datetime.strptime(event_time, '%Y-%m-%d %H:%M:%S')\n",
    "    shop[shopid].append((event_time, userid,orderid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e', 'd', 'c', 'b', 'a', 'f'}\n"
     ]
    }
   ],
   "source": [
    "# union 2 set\n",
    "x = {\"a\", \"b\", \"c\"}\n",
    "y = {\"f\", \"d\", \"a\"}\n",
    "z = {\"c\", \"d\", \"e\"}\n",
    "result = x.union(y, z)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy filter dataframe by between time :\n",
    "# q is a dataframe\n",
    "q[q.event_time.between(r.event_time, end_time)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
